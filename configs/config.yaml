defaults:
  - _self_
  - experiments: test_toxicity #generation # test_toxicity
  - peft_models
  - training
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

hydra:
  output_subdir: null
  run:
    dir: .

# Global logging configuration
logging:
  use_wandb: true
  entity: LLM_Accountability  
  wandb_project_name: # gets overwritten by experiment config
  quiet: true # logging to file only


# Default metric configuration
metric:
  behavior: toxicity
  metric: toxicity # toxicity # perspective
  lower_lim: 0.0
  upper_lim: 1.0
  dataset_name: allenai/real-toxicity-prompts
  dataset_split: train
  few_shot: false

# Default configurations for tau models using YAML anchors
tau_defaults: &tau_defaults
  model_kwargs:
    quantization_config:
      load_in_4bit: true
      bnb_4bit_quant_type: "nf4"
      bnb_4bit_compute_dtype: "bfloat16"
      bnb_4bit_use_double_quant: true
    low_cpu_mem_usage: true
    device_map: auto
    torch_dtype: "torch.bfloat16"  # Use bfloat16 to save memory
    # Additional memory optimizations
    max_memory: null  # Let device_map handle memory distribution
    offload_folder: null  # Only used if you want to offload to disk
  default_gen_kwargs: &default_gen_kwargs
    max_new_tokens: 100
    do_sample: true
    temperature: 0.7
    top_p: 0.9
  gen_kwargs: *default_gen_kwargs

# tau1 configuration, inheriting from tau_defaults
tau1:
  <<: *tau_defaults
  model_id: Meta-Llama-3-8B-Instruct
  gen_seed: seed1000
  hf_prefix: meta-llama

# Default output_path
storing:
  dir_prefix: ./data/behavior_data/${metric.metric} # base dir for everything
  output_dir: model_outputs
  score_dir: model_scores
  test_dir: test_outputs
  plot_dir: plots

# for debugging
debug_mode: false
