# @package _global_

exp: test

# Override tau1 configurations
tau1:
  model_id: Mistral-7B-Instruct-v0.2
  #Meta-Llama-3-8B-Instruct #Mistral-7B-Instruct-v0.2 #gemma-1.1-7b-it
  #Llama-3-8B-ckpt1
  gen_seed: seed1000

tau2:
  model_kwargs: ${tau_defaults.model_kwargs}
  default_gen_kwargs: ${tau_defaults.default_gen_kwargs}
  gen_kwargs: ${tau_defaults.gen_kwargs}
  model_id: Mistral-7B-Instruct-v0.2
  #Meta-Llama-3-8B-Instruct #Mistral-7B-Instruct-v0.2 #gemma-1.1-7b-it
  #Llama-3-8B-ckpt1 #Meta-Llama-3-8B-Instruct_hightemp
  gen_seed: seed2000

metric:
  behavior: toxicity
  metric: perspective #perspective # toxicity for Roberta-based toxicity classifier, see https://huggingface.co/spaces/evaluate-measurement/toxicity
  dataset_name: allenai/real-toxicity-prompts

test_params:
  only_continuations: true # whether to score only model generations or whole prompt + generation
  fold_size: 4000 # number of samples per fold. use larger number to test false positives
  overwrite: false
  calibrate: false # whether to do a calibration step first to get an estimate of epsilon
  noise: 0 # noise for behavior scores
  track_c2st: false # track C2ST for this experiment
  run_davtt: true
  analyze_distance: true
  calibrate_only: false
  calibration_params:
    epsilon_strategy: default # std # interval # if calibrate==True, strategy to derive epsilon in test
    epsilon_ticks: 10 # if calibrate==True, number of epsilon values to try
    lower_interval_end: 0
    upper_interval_end: 0.2
    lower_model_name: Llama-3-8B-ckpt1 #Meta-Llama-3-8B-Instruct_hightemp
    lower_model_seed: seed1000
    upper_model_name: Llama-3-8B-ckpt10 #LLama-3-8b-Uncensored
    upper_model_seed: seed1000
    num_runs: 20
  epsilon: 0
  analysis_params:
    calculate_distance: true
    unpaired: false
    num_runs: 20
    num_samples: -1
    multiples_of_epsilon: 3
    use_full_ds_for_nn_distance: false
    epsilon_ticks: 10

# Shared betting network configuration
net:
  input_size: 1
  hidden_layer_size: [32, 32]
  layer_norm: true
  bias: true
  drop_out: false
  drop_out_p: 0.0
  model_type: "CMLP" # Options: "CMLP", "HighCapacityCMLP"
  # Additional params for HighCapacityCMLP
  num_layers: 3
  batch_size: 64

training:
  seed: 0
  epochs: 100
  seqs: 60
  lr: 0.001
  batch_size: 100 # for sequences
  alpha: 0.05
  T: 0 # warm start
  save_dir: models
  save: true
  l1_lambda: 0.0
  l2_lambda: 0.0
  net_batch_size: 100 # for actual model training
  earlystopping:
    patience: 10
    delta: 0.0

logging:
  use_wandb: false
  wandb_project_name: Test_Toxicity
  quiet: false