# @package _global_

exp: test 

# Override tau1 configurations
tau1:
  model_id: Meta-Llama-3-8B-Instruct
  gen_seed: seed1000

# Define tau2 configuration, inheriting tau_defaults from base config
tau2:
  <<: ${tau_defaults}  # This will inherit from the base config tau_defaults
  model_id: Meta-Llama-3-8B-Instruct_fewshot
  gen_seed: seed1000

metric:
  behavior: translation_performance  
  metric: bleu #rouge                               
  dataset_name: allenai/real-toxicity-prompts  

test_params:
  only_continuations: true # whether to score only model generations or whole prompt + generation
  fold_size: 2000
  overwrite: false
  calibrate: false
  noise: 0
  track_c2st: true
  analyze_distance: true
  calibrate_only: false


# Shared betting network configuration
net:
  input_size: 1
  hidden_layer_size: [32, 32]
  layer_norm: true
  bias: true
  model_type: "CMLP"  # Options: "CMLP", "HighCapacityCMLP"
  # Additional params for HighCapacityCMLP
  num_layers: 3
  batch_size: 64

calibration_params: 
  epsilon_strategy: default # std # interval # if calibrate==True, strategy to derive epsilon in test
  epsilon_ticks: 10 # if calibrate==True, number of epsilon values to try
  bias: 0 # if calibrate==True, bias for epsilon values
  lower_interval_end: 0 
  upper_interval_end: 0.2
  lower_model_name: Meta-Llama-3-8B-Instruct_hightemp
  lower_model_seed: seed1000
  upper_model_name: LLama-3-8b-Uncensored
  upper_model_seed: seed1000
  num_runs: 20

epsilon: 0 # use exact test by default

# Analysis configuration
analysis:
  calculate_distance: true
  unpaired: false
  num_runs: 20
  num_samples: 0
  multiples_of_epsilon: 3
  bias: 0
  use_full_ds_for_nn_distance: false
  epsilon_ticks: 10

logging:
  use_wandb: false
  wandb_project_name: Test_Translation

